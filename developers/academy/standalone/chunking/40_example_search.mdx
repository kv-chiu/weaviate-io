---
title: Example part 2 - Search
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import FilteredTextBlock from '@site/src/components/Documentation/FilteredTextBlock';
import CodePracticalExample from '!!raw-loader!./_snippets/30_example.py';

import imageUrl from '../../tmp_images/academy_placeholder.jpg';

<img src={imageUrl} alt="Image alt" width="75%"/>

## <i class="fa-solid fa-square-chevron-right"></i>&nbsp;&nbsp;Overview

We have retrieved and imported multiple chapters of a book into Weaviate using different chunking techniques [earlier](./30_example_chunking.mdx). Now, we will use Weaviate to search through the book and evaluate the impact of the chunking techniques.

### <i class="fa-solid fa-chalkboard"></i>&nbsp;&nbsp;Recap

We've generated chunks using four different methods:

- Fixed-length chunks (and 20% overlap)
    - With 25 words per chunk, and
    - With 100 words per chunk
- Variable-length chunks, using paragraph markers, and
- Mixed-strategy chunks, using paragraph markers and a minimum chunk length of 25 words.

Since the data comes from the first two chapters of a book about Git, let's search for various git-related concepts and see how the different chunking strategies perform.


## <i class="fa-solid fa-square-chevron-right"></i>&nbsp;&nbsp;Search / recall

First of all, we'll retrieve information from our Weaviate instance using various search terms. We'll use a semantic search (`nearText`) to aim to retrieve the most relevant chunks.

### <i class="fa-solid fa-code"></i>&nbsp;&nbsp;Search syntax

The search is carried out as follows, looping through each chunking strategy by filtering our dataset. We'll obtain a couple of top results for each search term.

<Tabs groupId="languages">
<TabItem value="py" label="Python">
<FilteredTextBlock
  text={CodePracticalExample}
  startMarker="# START vector_search"
  endMarker="# END vector_search"
  language="py"
/>
</TabItem>
</Tabs>

Using these search terms:
- `"history of git"`
- `"how to add the url of a remote repository"`

### <i class="fa-solid fa-chalkboard"></i>&nbsp;&nbsp;Results & discussions

We get the following results:

#### Example 1

:::info Results for a search for `"history of git"`.
:::

<Tabs groupId="chunking">
<TabItem value="fixed_size_25" label="25 word chunks">
<FilteredTextBlock
  text={CodePracticalExample}
  startMarker="# START fixed_size_25 vector_search_history"
  endMarker="# END fixed_size_25 vector_search_history"
  language="text"
/>
</TabItem>
<TabItem value="fixed_size_100" label="100 word chunks">
<FilteredTextBlock
  text={CodePracticalExample}
  startMarker="# START fixed_size_100 vector_search_history"
  endMarker="# END fixed_size_100 vector_search_history"
  language="text"
/>
</TabItem>
<TabItem value="para_chunks" label="Paragraph chunks">
<FilteredTextBlock
  text={CodePracticalExample}
  startMarker="# START para_chunks vector_search_history"
  endMarker="# END para_chunks vector_search_history"
  language="text"
/>
</TabItem>
<TabItem value="para_chunks_min_25" label="Paragraph chunks with minimum length">
<FilteredTextBlock
  text={CodePracticalExample}
  startMarker="# START para_chunks_min_25 vector_search_history"
  endMarker="# END para_chunks_min_25 vector_search_history"
  language="text"
/>
</TabItem>
</Tabs>

The query in this example is a broad one on the `history of git`. The result is that here, the longer chunks seem to perform better.

Inspecting the result, we see that while the 25-word chunks may be semantically similar to the query `history of git`, they do not contain enough contextual information to enhance the readers' understanding of the topic.

On the other hand, the paragraph chunks retrieved - especially those with a minimum length of 25 words - contain a good amount of holistic information that will teach the reader about the history of git.

#### Example 2

:::info Results for a search for `"how to add the url of a remote repository"`.
:::

<Tabs groupId="chunking">
<TabItem value="fixed_size_25" label="25 word chunks">
<FilteredTextBlock
  text={CodePracticalExample}
  startMarker="# START fixed_size_25 vector_search_remote_repo"
  endMarker="# END fixed_size_25 vector_search_remote_repo"
  language="text"
/>
</TabItem>
<TabItem value="fixed_size_100" label="100 word chunks">
<FilteredTextBlock
  text={CodePracticalExample}
  startMarker="# START fixed_size_100 vector_search_remote_repo"
  endMarker="# END fixed_size_100 vector_search_remote_repo"
  language="text"
/>
</TabItem>
<TabItem value="para_chunks" label="Paragraph chunks">
<FilteredTextBlock
  text={CodePracticalExample}
  startMarker="# START para_chunks vector_search_remote_repo"
  endMarker="# END para_chunks vector_search_remote_repo"
  language="text"
/>
</TabItem>
<TabItem value="para_chunks_min_25" label="Paragraph chunks with minimum length">
<FilteredTextBlock
  text={CodePracticalExample}
  startMarker="# START para_chunks_min_25 vector_search_remote_repo"
  endMarker="# END para_chunks_min_25 vector_search_remote_repo"
  language="text"
/>
</TabItem>
</Tabs>

The query in this example was a more specific one, for example one that might be run by a user looking to identify how to add the url of a remote repository.

In contrast to the first scenario, the 25-word chunks are more useful here. Because the question was very specific, Weaviate was able to identify the chunk containing the most suitable passage - how to add a remote repository (`git remote add <shortname> <url>`).

While the other result sets also contain some of this information, it may be worth considering how the result may be used and displayed. The longer the result, the more cognitive effort it may take the user to identify the relevant information.


## <i class="fa-solid fa-square-chevron-right"></i>&nbsp;&nbsp;Generative search

Now, let's tackle the topic of generative search. Generative search allows you to use Weaviate to use retrieved objects as a part of inputs to a generative model.

In the context of chunking, the chunk size determines the breadth of the information that is retrieved. This is due to the fact that generative models currently have a limited maximum input size, called the context window. As a result, the larger the chunk size, fewer chunks can be included in the context window.

For example, using shorter (e.g. 25-word) chunks will allow you to retrieve information from a wider range of book locations than longer chunks. Let's try a few generative search examples to see how this manifests itself.

### <i class="fa-solid fa-code"></i>&nbsp;&nbsp;Search syntax

The generative search syntax is shown below. The syntax is largely the same as above, except for two aspects.

One is that to account for varying chunk sizes, we will retrieve more chunks where the chunk size is smaller.

The other is that an additional generative search operator is added to the search query. The query asks the target LLM to summarize the results into point form.

<Tabs groupId="languages">
<TabItem value="py" label="Python">
<FilteredTextBlock
  text={CodePracticalExample}
  startMarker="# START generative_search"
  endMarker="# END generative_search"
  language="py"
/>
</TabItem>
</Tabs>

### <i class="fa-solid fa-chalkboard"></i>&nbsp;&nbsp;Results & discussions

#### Example 1

:::info Results for a search for `"history of git"`.
:::

<Tabs groupId="chunking">
<TabItem value="fixed_size_25" label="25 word chunks">
<FilteredTextBlock
  text={CodePracticalExample}
  startMarker="# START fixed_size_25 generative_search_git_history"
  endMarker="# END fixed_size_25 generative_search_git_history"
  language="text"
/>
</TabItem>
<TabItem value="fixed_size_100" label="100 word chunks">
<FilteredTextBlock
  text={CodePracticalExample}
  startMarker="# START fixed_size_100 generative_search_git_history"
  endMarker="# END fixed_size_100 generative_search_git_history"
  language="text"
/>
</TabItem>
<TabItem value="para_chunks" label="Paragraph chunks">
<FilteredTextBlock
  text={CodePracticalExample}
  startMarker="# START para_chunks generative_search_git_history"
  endMarker="# END para_chunks generative_search_git_history"
  language="text"
/>
</TabItem>
<TabItem value="para_chunks_min_25" label="Paragraph chunks with minimum length">
<FilteredTextBlock
  text={CodePracticalExample}
  startMarker="# START para_chunks_min_25 generative_search_git_history"
  endMarker="# END para_chunks_min_25 generative_search_git_history"
  language="text"
/>
</TabItem>
</Tabs>

The findings here are similar to the semantic search results. The longer chunks contain more information, and are more useful for a broad topic like the history of git.

#### Example 2

:::info Results for a search for `"available git remote commands"`.
:::

<Tabs groupId="chunking">
<TabItem value="fixed_size_25" label="25 word chunks">
<FilteredTextBlock
  text={CodePracticalExample}
  startMarker="# START fixed_size_25 generative_search_git_remote"
  endMarker="# END fixed_size_25 generative_search_git_remote"
  language="text"
/>
</TabItem>
<TabItem value="fixed_size_100" label="100 word chunks">
<FilteredTextBlock
  text={CodePracticalExample}
  startMarker="# START fixed_size_100 generative_search_git_remote"
  endMarker="# END fixed_size_100 generative_search_git_remote"
  language="text"
/>
</TabItem>
<TabItem value="para_chunks" label="Paragraph chunks">
<FilteredTextBlock
  text={CodePracticalExample}
  startMarker="# START para_chunks generative_search_git_remote"
  endMarker="# END para_chunks generative_search_git_remote"
  language="text"
/>
</TabItem>
<TabItem value="para_chunks_min_25" label="Paragraph chunks with minimum length">
<FilteredTextBlock
  text={CodePracticalExample}
  startMarker="# START para_chunks_min_25 generative_search_git_remote"
  endMarker="# END para_chunks_min_25 generative_search_git_remote"
  language="text"
/>
</TabItem>
</Tabs>

The results of the generative search here for `available git remote commands` are perhaps even more illustrative than before.

Here, the shortest chunks were able to retrieve the highest number of `git remote` commands from the book. This is because we were able to retrieve more chunks from various locations throughout the corpus (book). This is in contrast to the longer chunks, which were only able to retrieve fewer commands.

Of course, it is likely that the longer chunks are able to retrieve more information about each command. But this is a trade-off that you will need to consider when deciding on the chunking strategy for your use case.

## <i class="fa-solid fa-square-chevron-right"></i>&nbsp;&nbsp;Review

<Quiz questions={varName} />

Any quiz questions

### <i class="fa-solid fa-pen-to-square"></i>&nbsp;&nbsp;Review exercise

:::note <i class="fa-solid fa-square-terminal"></i> Exercise
Try out ...
:::

### <i class="fa-solid fa-lightbulb-on"></i>&nbsp;&nbsp;Key takeaways

:::info
Add summary
:::

import { GiscusDocComment } from '/src/components/GiscusComment';

<GiscusDocComment />

import Quiz from '/src/components/Academy/quiz.js'
const varName = [{
  questionText: 'questionText',
  answerOptions: [
    {
      answerText: 'answerOne',
      isCorrect: false,
      feedback: 'feedbackOne',
    },
    {
      answerText: 'answerTwo',
      isCorrect: false,
      feedback: 'feedbackTwo',
    },
    {
      answerText: 'answerThree',
      isCorrect: false,
      feedback: 'feedbackThree',
    },
  ]
}];