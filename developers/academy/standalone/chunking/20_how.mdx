---
title: Chunking techniques
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import FilteredTextBlock from '@site/src/components/Documentation/FilteredTextBlock';
import CodeFixedSizeChunking from '!!raw-loader!./_snippets/20_chunking_methods.1.fixed.size.py';
import CodeVariableSizeChunking from '!!raw-loader!./_snippets/20_chunking_methods.2.variable.size.py';
import CodeMixedStrategyChunking from '!!raw-loader!./_snippets/20_chunking_methods.3.mixed.strategy.py';

import imageUrl from '../../tmp_images/academy_placeholder.jpg';

<img src={imageUrl} alt="Image alt" width="75%"/>

## <i class="fa-solid fa-square-chevron-right"></i>&nbsp;&nbsp;Overview

Now that you've learned about what chunking is, and why it is important, let's look at some common chunking techniques. We'll take you through **fixed-size** and **variable-size** chunking techniques, including some example implementations of each.

## <i class="fa-solid fa-square-chevron-right"></i>&nbsp;&nbsp;Fixed-size chunking

As the name suggests, fixed-size chunking refers to the process of splitting texts into chunks of a fixed size. This may be the most common chunking technique due to its simplicity and effectiveness.

### <i class="fa-solid fa-chalkboard"></i>&nbsp;&nbsp;Implementations

Fixed-size chunking is implemented by splitting texts into chunks of a fixed number of units. The units may be composed of words, characters, or even *tokens*, and the number of units per chunk is fixed (to a maximum), with an optional overlap.

:::tip What is a token?
A "token" in this context is a unit of text that will be processed by a model by being substituted with a number. In modern tranformer models, a token is commonly a "subword" unit composed of a few characters.
:::

One pseudocode implementation of fixed-size chunking is:

```python
# Given a text of length L
# Split the text into chunks of size N units (e.g. tokens, characters, words)
# Optionally, add an overlap of M units at the beginning or end of each chunk (from the previous or next chunk)
# This should typically result in a list of chunks of length L // N + 1
```

And implementing in Python, it may look like:

<Tabs groupId="languages">
<TabItem value="py" label="Python">
<FilteredTextBlock
  text={CodeFixedSizeChunking}
  startMarker="# START Vanilla fixed size chunker"
  endMarker="# END Vanilla fixed size chunker"
  language="py"
/>
</TabItem>
</Tabs>

Which can be modified to include an overlap (in this case, at the beginning of each chunk):

<Tabs groupId="languages">
<TabItem value="py" label="Python">
<FilteredTextBlock
  text={CodeFixedSizeChunking}
  startMarker="# START Fixed size chunker with overlap"
  endMarker="# END Fixed size chunker with overlap"
  language="py"
/>
</TabItem>
</Tabs>

This is far from the only way to implement fixed-size chunking, but it is one possible, relatively simple, implementation.

:::note Exercise
Consider how *you* might implement fixed-size chunking. What would your pseudocode (or code) look like?
:::

### <i class="fa-solid fa-code"></i>&nbsp;&nbsp;Examples

We are ready to look at some concrete examples of fixed-size chunking. Let's take a look at three examples, with a chunk size of 5 words, 25 words and 100 words, respectively.

We'll use an excerpt from the [Pro Git book](https://git-scm.com/book/en/v2)*. More specifically, we'll use text of the [What is Git?](https://github.com/progit/progit2/blob/main/book/01-introduction/sections/what-is-git.asc) chapter.

:::info Pro Git by Scott Chacon and Ben Straub - Book License

<small>*Available through the <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/">Creative Commons Attribution-Non Commercial-Share Alike 3.0 license</a>.</small>

:::

Here is one example using our chunking function from above:

<Tabs groupId="languages">
<TabItem value="py" label="Python">
<FilteredTextBlock
  text={CodeFixedSizeChunking}
  startMarker="# START Get fixed-size chunks examples"
  endMarker="# END Get fixed-size chunks examples"
  language="py"
/>
</TabItem>
</Tabs>

This will result in outputs like these. Take a look at the first few chunks at each size - what do you notice?

:::note Exercise
Consider which of these chunk sizes would be most appropriate for search. Why do you think so? What are the tradeoffs?
:::

<Tabs groupId="languages">
<TabItem value="5" label="By 5 words">
<FilteredTextBlock
  text={CodeFixedSizeChunking}
  startMarker="# START Chunking by 5 words - outputs"
  endMarker="# END Chunking by 5 words - outputs"
  language="text"
/>
</TabItem>
<TabItem value="25" label="By 25 words">
<FilteredTextBlock
  text={CodeFixedSizeChunking}
  startMarker="# START Chunking by 25 words - outputs"
  endMarker="# END Chunking by 25 words - outputs"
  language="text"
/>
</TabItem>
<TabItem value="100" label="By 100 words">
<FilteredTextBlock
  text={CodeFixedSizeChunking}
  startMarker="# START Chunking by 100 words - outputs"
  endMarker="# END Chunking by 100 words - outputs"
  language="text"
/>
</TabItem>
</Tabs>


Hopefully, these concrete examples start to illustrate some of the ideas that we discussed above.

Immediately, it strikes me that the smaller chunks are very granular, to the point where they may not contain enough information to be useful for search. On the other hand, the larger chunks begin to retain more information as they get to lengths that are similar to a typical paragraph.

Now imagine these chunks becoming even longer. As chunks become longer, the corresponding vector embeddings would start to become more general. This would eventually reach a point where they cease to be useful in terms of searching for information.

:::note What about character or sub-word tokenization?
At these sizes, you typically will not need to employ character-based or sub-word token-based chunking, as splitting words at these boundaries in a group of words will not typically be meaningful.
:::

:::tip Where to start?
For search with fixed-size chunks, if you don't have any other factors, try a size of around 100-200 words, and a 20% overlap.
:::

## <i class="fa-solid fa-square-chevron-right"></i>&nbsp;&nbsp;Variable-size chunking

Now let's look at variable-size chunking. Unlike fixed-size chunking, the chunk size here is an *outcome*, rather than an *input parameter*.

In variable-size chunking, some marker is used to split the text. The marker may be syntactic, such as a sentence or paragraph marker, or even structural such as a markdown header.

### <i class="fa-solid fa-chalkboard"></i>&nbsp;&nbsp;Implementations

A pseudocode implementation of variable-size chunking could look like this:

```python
# Given a text of length L
# Identify a marker (e.g. full-stop, paragraph marker (two newlines), or a Markdown header
# Split the text into chunks at each marker
```

Which could be implemented in Python as below:

<Tabs groupId="languages">
<TabItem value="py" label="Python">
<FilteredTextBlock
  text={CodeVariableSizeChunking}
  startMarker="# START Paragraph variable size chunker"
  endMarker="# END Paragraph variable size chunker"
  language="py"
/>
</TabItem>
</Tabs>

Or, we could use special markers - such as Markdown headers - to split the text.

(Since the *Pro Git* book is written in [Asciidoc](https://asciidoc.org/), we'll use Asciidoc headers instead - they all start with new lines followed by `==`.)

<Tabs groupId="languages">
<TabItem value="py" label="Python">
<FilteredTextBlock
  text={CodeVariableSizeChunking}
  startMarker="# START Asciidoc section variable size chunker"
  endMarker="# END Asciidoc section variable size chunker"
  language="py"
/>
</TabItem>
</Tabs>

Again, let's now apply these to concrete examples.

### <i class="fa-solid fa-code"></i>&nbsp;&nbsp;Examples

We can apply these splitters to the same text as before.

<Tabs groupId="languages">
<TabItem value="py" label="Python">
<FilteredTextBlock
  text={CodeVariableSizeChunking}
  startMarker="# START Get variable-size chunks examples"
  endMarker="# END Get variable-size chunks examples"
  language="py"
/>
</TabItem>
</Tabs>

And the outputs look like this. Now, what do you observe?

<Tabs groupId="languages">
<TabItem value="para" label="By paragraph">
<FilteredTextBlock
  text={CodeVariableSizeChunking}
  startMarker="# START Chunking by paragraph - outputs"
  endMarker="# END Chunking by paragraph - outputs"
  language="text"
/>
</TabItem>
<TabItem value="header" label="By header">
<FilteredTextBlock
  text={CodeVariableSizeChunking}
  startMarker="# START Chunking by header - outputs"
  endMarker="# END Chunking by header - outputs"
  language="text"
/>
</TabItem>
</Tabs>

One thing that might stand out immediately is that both of our very simple marker-based chunker ends up extracting the heading as one chunk, which may not be desirable.

In reality, you may employ a mixed strategy where very short chunks like this may be appended to the next chunk, assuming that it is likely to be something like a title, or a section heading.

Let's take a look at such a strategy.


## <i class="fa-solid fa-square-chevron-right"></i>&nbsp;&nbsp;Mixed strategy

You could use a mix of fixed-size chunking and variable-size chunking to get the best of both worlds. For example, you could use a variable-size chunker to split the chunks at paragraph markers, but apply a fixed-size filter.

More specifically, any chunks that are too small could be merged with the next chunk, and/or any chunks that are too large could be split at the middle, or at another marker within the chunk.

### <i class="fa-solid fa-code"></i>&nbsp;&nbsp;Examples

One implementation may look as follows:

```python
# Given a text of length L
# Identify a marker (e.g. full-stop, paragraph marker (two newlines), or a Markdown header
# Split the text into chunks at each marker
# If any of the chunks are too small, merge them with the next chunk
# If any of the chunks are too large, split them - e.g. at the middle or using another marker within the chunk
```

Which could be implemented in Python like this:

<Tabs groupId="languages">
<TabItem value="py" label="Python">
<FilteredTextBlock
  text={CodeMixedStrategyChunking}
  startMarker="# START Asciidoc and size based chunking"
  endMarker="# END Asciidoc and size based chunking"
  language="py"
/>
</TabItem>
</Tabs>

Producing these chunks.

<FilteredTextBlock
  text={CodeMixedStrategyChunking}
  startMarker="# START Mixed-strategy chunking output"
  endMarker="# END Mixed-strategy chunking output"
  language="text"
/>

This strategy will not produce chunks that are too small, while still basing them based on a syntactic marker, respecting the boundary of a heading.

Since we've seen chunking strategies in action on a single text, let's now look at how they may work on a larger set of texts. We'll also take a look at what retrieval results may look like, using different chunking strategies.


## <i class="fa-solid fa-square-chevron-right"></i>&nbsp;&nbsp;Review

<Quiz questions={varName} />

Any quiz questions

### <i class="fa-solid fa-pen-to-square"></i>&nbsp;&nbsp;Review exercise

:::note <i class="fa-solid fa-square-terminal"></i> Exercise
Try out ...
:::

### <i class="fa-solid fa-lightbulb-on"></i>&nbsp;&nbsp;Key takeaways

:::info
Add summary
:::

import { GiscusDocComment } from '/src/components/GiscusComment';

<GiscusDocComment />

import Quiz from '/src/components/Academy/quiz.js'
const varName = [{
  questionText: 'questionText',
  answerOptions: [
    {
      answerText: 'answerOne',
      isCorrect: false,
      feedback: 'feedbackOne',
    },
    {
      answerText: 'answerTwo',
      isCorrect: false,
      feedback: 'feedbackTwo',
    },
    {
      answerText: 'answerThree',
      isCorrect: false,
      feedback: 'feedbackThree',
    },
  ]
}];